% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage[utf8]{inputenc}
\usepackage{graphicx,subfig}
\usepackage{amstext,amsmath,amssymb,bm,bbm,mathtools}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage[export]{adjustbox}
\usepackage[dvipsnames]{xcolor}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%

\definecolor{violet}{rgb}{0.56, 0.0, 1.0}

\newcommand{\todo}[1]{{\textcolor{blue}{#1}}}
\newcommand{\test}[1]{{\textcolor{red}{#1}}}
\newcommand{\jaco}[1]{{\textcolor{green!50!black}{#1}}}
\newcommand{\HT}[1]{{\textcolor{violet}{#1}}}
\newcommand{\daniel}[1]{\textcolor{NavyBlue}{#1}}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{An Elastica-driven Digital Curve Evolution Model for Image Segmentation\thanks{This  work has  been  partially  funded by CoMeDiC ANR-15-CE40-0006 research grant.}}
  \titlerunning{Digital Curve Evolution by Elastica Energy}
  %\title{Digital Curvature Evolution Models for Image Segmentation\thanks{This  work has  been  partially  funded by CoMeDiC ANR-15-CE40-0006 research grant.}}

\author{Daniel Antunes\inst{1}
Jacques-Olivier Lachaud\inst{1}
Hugues Talbot\inst{2}}
%
\authorrunning{D. Antunes et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Universit{\'e} Savoie Mont Blanc, LAMA, UMR CNRS 5127, F-73376, France
\email{daniel.martins-antunes, jacques-olivier.lachaud@univ-savoie.fr} \and
CentraleSupelec Inria \'equipe OPIS, Universit\'e Paris-Saclay, 9 rue Joliot-Curie, F-91190 Gif-sur-Yvette \\
\email{hugues.talbot@centralesupelec.fr}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
  Geometric priors have been shown to be useful in image segmentation to regularize results. For example, the classical
    Mumford-Shah functional uses regions perimeter as prior. This has inspired much research in the last few decades,
    with classical approaches like the Rudin-Osher-Fatemi and most graph-cut formulations, which all use a weighted or
    binary perimeter prior. It has been observed that this prior is not suitable in many applications, for example for
    segmenting thin objects or some textures, which may have high perimeter/surface ratio. Mumford observed that an
    interesting prior for natural objects is the Euler Elastical model, which involves the squared curvature. In other
    areas of science, researchers have noticed that some physical binarization processes, like emulsion unmixing can be
    well-approximated by curvature-related flow like the Willmore flow. 
    However, curvature-related flows are not easy to compute because curvature is difficult to estimate accurately, and the
    underlying optimisation processes are not convex. In this article, we propose to formulate a digital flow that
    approximates a curvature-related flow using a multigrid-convergent curvature estimator, within a discrete
    variational framework. We also present an application of this model as a post-processing step in a segmentation
    framework.
\keywords{Multigrid convergence  \and Digital estimator \and Curvature \and Shape Optimization \and Image Segmentation.}
\end{abstract}
%
%
%
\setcounter{footnote}{0}
\section{Introduction} %% \todo{(To be extended)}}


Geometric quantities are particularly useful as regularizers in
low-level image analysis, especially when little prior information is
known about the shape of interest. Length penalization is a well-behaved, general
purpose regularizer and many models in the literature make use
of it, from active contours \cite{casseles97geodesic} to level-set
formulations \cite{malladi1995image,malladi1995shape}. Discrete
graph-based variational models have been particularly succesfull to
incorporate length penalization, while keeping the
ability to extract a global optimum
\cite{boykov01graphcut,appleton05geodesic}.

However length regularization shows limitations when segmenting
small or thin and elongated objects, as it tends to shrink solutions
or yields disconnected solutions. Such drawbacks can be problematic in
image segmentation or image inpainting. It is thus rather natural to
consider curvature (especially squared curvature) as a potential
regularizer. Energies involving both length and squared curvature are often called
{\em Euler elastica} model (that dates back to Euler) or {\em Willmore energy}
($n$-dimensional extension) and have been brought back to attention in
computer vision by Mumford \cite{mumford1994elastica}. A similar
regularizer is also present in the second-order regularizer
of the original snake model \cite{kass1988snakes}. Similarly,
an explicit curvature term was often employed in early level-set methods,
like~\cite{malladi1995image,malladi1995shape,ballester01filljoint} for segmentation or inpainting.
However, Casselles {\em et al.} in~\cite{casseles97geodesic} observed that these
terms were not geometric, in the sense that they depended on the discretization.

The use of curvature for surface regularization was studied in~\cite{bobenko2005discrete},
although results look promising, authors used an inexact linearized version of the
square curvature at every step.

One of the first successful uses of curvature in image processing is the inpainting algorithm
described in \cite{masnou98inpainting}, where authors evaluate the absolute curvature along the level lines of a simply-connected image to reconstruct its occluded parts. The non-intersection property of level lines induces the
construction of an efficient dynamic programming algorithm. However,  the curvature energy is only coarsely
approximated.  In \cite{chan02elasticainpainting}, a geometric inpainting method involving elastica
optimized with level-sets is described.

In image segmentation, Zehiry and Grady have shown that injecting curvature as a regularizer can help recover thin
and elongated objects \cite{zehiry10fast}. Similarly Schoenemann {\em et al.}  \cite{schoenemann09linear} have
considered the elastica energy in their ratio-based image segmentation approach. In~\cite{schoenemann2011elastic},
Schoenemann~{\em et al.} extend ratio-cut approaches to segmentation with a curvature term. Their global optimization
framework shows the considerable advantages of using such regularizer in common binary image segmentation tasks.
However the time complexity of their algorithm is prohibitive, even if curvature is again coarsely
approximated.  In more recent work, Lim {\em et al.} propose an edge-weighted Willmore-like flow to segment
vertebra~\cite{lim2012introducing} using a level-set formulation.  They observe improved results compared
to~\cite{casseles97geodesic} and other perimeter-based priors, but observe that a shape prior and a close-enough
solution must also be provided to achieve good results.

In fact, it is still a very challenging task to inject curvature in
the context of image segmentation.  
State-of-art methods are in practice difficult to optimize and do not scale
\cite{zehiry10fast,schoenemann09linear,strandmark11globalframework,nieuwenhuis14efficient}. In
order to achieve reasonable running times, such approaches make use of
coarse curvature estimations for which the approximation error is
often unknown. Improving the quality of the curvature estimator has an
important impact on the accuracy of the results, but is
computationally too costly in these methods.

Segmentation methods using Elastica or Willmore energies generally
reduce to computing the corresponding flow, i.e. following its
gradient. Well-founded level-set methods involving Willmore energy
\cite{droske2004level} have been used in medical image segmentation
\cite{lim2012introducing}. Another level-set formulation similar to
the Chan-Vese model was proposed in \cite{zhu2013image}. Willmore
flow can also be carried out by phase-field models (see
\cite{bretin2015phase}) but they are less suited to image segmentation
since interfaces are blurred in such models. Threshold dynamics can
also be considered for such energies \cite{esedoglu2008threshold} and
has been proposed for image desocclusion \cite{esedoglu2005threshold}.

In this paper we are interested in purely discrete variational segmentation models involving an elastica
energy. Recently, new estimators of curvature along digital contours have been proposed
\cite{roussillon11mdca,coeurjolly13integral,schindele17mdca} with the desirable multigrid convergence property. This
motivates us to propose models in which they can be used successfully.

In the remainder, we investigate the use of the digital integral invariant
curvature estimator \cite{coeurjolly13integral} in a discrete
variational segmentation framework. More precisely, we show how to
incorporate it in a digital flow minimizing its squared
curvature. This model is expressible as a discrete combinatorial
optimization model, which we solve using a classical variant of QPBO
algorithm \cite{rother07qpbo}. Our solution does not compute a global
solution to the whole domain, but our approach leads to a digital flow
similar to continuous Elastica, as shown by our experiments. We
present an application of the model as a post-processing step in a
segmentation framework, and demonstrate how it can improve standard
results. \daniel{This paper is an extended version of a previous work that appeared in \cite{antunes19}. We've included a deeper discussion on the optimization method we've used and we present a comparison of our segmentation method with two other related works, and illustrate the results with several examples. Moreover, we have included a new local combinatorial optimization approach for elastica minimization. Finnaly,  the code produced in this paper is freely available on github\footnote{https://www.github.com/danoan/BTools}.}



\textit{Outline}. Section two reviews the concept of multigrid
convergence and highlights its importance for the definition of
digital estimators. Next, we describe two convergent estimators used
in this paper, one for tangent and the other for curvature. They are
used in the optimization model and in the definition of the digital
elastica. Section three describes a local combinatorial optimization model
suitable for different curvature estimators. Section four describes the proposed 
curvature evolution model along with several illustrations of digital flows. Section five
explains how to use the evolution model as a post-processing step in
an image segmentation framework. 



\section{Multigrid convergent estimators}

The goal of this section is to  introduce the concept of multigrid convergenc and its potential when analyzing digital images with variational models involving geometric quantities. More precisely we wish to delineate shapes within digital images with some priors related to their unknown continuous geometry. Of course we only observe shape digitizations within images.


A digital shape is the result of some quantization process over an
object $X$ lying in some continuous space of dimension $2$ (here).
For example, the Gauss digitization of a continuous subset $X$ of the
Euclidean plane $\mathbb{R}^2$ with grid step $h>0$ is defined as
\begin{align*}
	D_h(X) = X \cap (h\mathbb{Z})^2.
\end{align*} 

Given a shape $X$ and its digitization $D_h(X)$, a digital estimator $\hat{u}$ for some geometric quantity $u$ is
intended to compute $u(X)$ by using only the digitization. This problem is not well-posed, as the same digital object
could be the digitization of infinitely many objects very different from $X$. Therefore, a characterization of what constitutes
a good estimator is necessary.

Let $u$ be some geometric quantity of $X$ (e.g. tangent, curvature). We wish to devise a digital estimator $\hat{u}$ for
$u$. It is reasonable to state that $\hat{u}$ is a good estimator if $\hat{u}(D_h(X))$ converges to $u(X)$ as we refine
our grid. For example, counting pixels is a convergent estimator for area (with a rescale of $h^2$); but counting
boundary pixels (with a rescale of $h$) is not a convergent estimator for perimeter. Multigrid convergence is the
mathematical tool that makes this definition precise. Given any subset $Z$ of $(h\mathbb{Z})^2$, we can represent it as a
union of axis-aligned squares with edge length $h$ centered on the point of $Z$. The topological boundary of this union
of cubes is called {\em $h$-frontier} of $Z$. When $Z=D_h(X)$, we call it {\em $h$-boundary of $X$} and denote it by
$\partial_h X$.
%% In the following, let the $h$-frontier of $D_h(X)$ to be defined as $\partial D_h(X) = \partial \left( \frac{1}{h} \cdot X \right) \cap \mathbb{Z}^2$.

\begin{definition}[Multigrid convergence for local geometric quantites]
  A local discrete geometric estimator $\hat{u}$ of some geometric
  quantity $u$ is (uniformly) multigrid convergent for the family $\mathbb{X}$ if
  and only if, for any $X \in \mathbb{X}$, there exists a grid step
  $h_X>0$ such that the estimate $\hat{u}(D_h(X),\hat{x},h)$ is
  defined for all $\hat{x} \in \partial_hX$ with $ 0 < h < h_X$, and
  for any $x \in \partial X$,
  \begin{equation*}
    \forall \hat{x} \in  \partial_hX \text{ with } \norm{ \hat{x} - x }_{\infty} \leq h, \norm{ \hat{u}(D_h(X),\hat{x},h) - u(X,x)} \leq \tau_{X}(h),			
  \end{equation*}
  where $\tau_{X}:\mathbb{R}^{+}\setminus\{0\} \rightarrow
  \mathbb{R}^{+}$ has null limit at $0$. This function defines the
  speed of convergence of $\hat{u}$ towards $u$ for $X$.
\end{definition}
	
For a global geometric quantity (e.g. perimeter, area, volume), the definition remains the same, except that the mapping
between $\partial X$ and $\partial_h X$ is no longer necessary.
	
Multigrid convergent estimators provide a quality guarantee and should
be preferred over non-multigrid convergent ones. In the next section,
we describe two estimators that are important for our purpose.

\subsection{Tangent and Perimeter Estimators}

The literature presents several perimeter estimators that are multigrid convergent (see
\cite{coeurjolly04comparative,coeurjolly12multigrid} for a review), but in order to define the digital elastica we need a local estimation
of length and we wish that integration over these local length elements gives a multigrid convergent estimator for the
perimeter.

\begin{definition}[Elementary Length]
  Let a digital curve $C$ be represented as a sequence of grid vertices in a grid cell representation of digital objects (in a grid with step $h$). Further, let $\vec{ \hat{v} }$ be a multigrid convergent estimator for the unit tangent vector. The elementary length $\hat{s}(\vec{e})$ at some oriented grid edge $ \vec{e} \in C$ is defined as
  \begin{align*}
    \hat{s}(\vec{e}) = h ~\vec{\hat{v}}(\vec{e}) \cdot \vec{e}.
  \end{align*}
\end{definition}
The integration of the elementary length along the digital curve is a multigrid convergent estimator for perimeter if
one uses the $\lambda$-MST \cite{lachaud07tangent} tangent estimator (see \cite{lachaud06hdr}).

\subsection{Integral Invariant Curvature Estimator}
Generally speaking, an invariant is a function whose value is unaffected by the action
of some group on the elements of the domain.
%% Jaco: trop général, et pas utilisé après
%% Generally speaking, an invariant $\sigma$ is a real-valued function from some space $\Omega$ which value is unaffected by the action
%% of some group $\mathfrak{G}$ on the elements of the domain
%% \begin{align*}
%%   x \in \Omega, g \in \mathfrak{G}, \sigma(x) = v \longleftrightarrow \sigma(g \cdot x ) = v.
%% \end{align*}
Perimeter and curvature are examples of invariants for shapes on
$\mathbb{R}^2$ with respect to the euclidean group of rigid
transformations. Definition of integral area invariant and its link
with curvature is proven in \cite{manay04intinvariant}.


\begin{definition}[Integral area invariant]
  Let $X \subset \mathbb{R}^2$ and $B_r(p)$ the ball of radius $r$ centered at point $p$. Further, let
  $\mathbbm{1}_X(\cdot)$ be the characteristic function of $X$. The integral area invariant $\sigma_{X,r}(\cdot)$ is
  defined as
  \begin{align*}
    \forall p \in \partial X, \quad \sigma_{X,r}(p) = \int_{B_r(p)}{ \mathbbm{1}_X(x) dx}.
  \end{align*}
\end{definition}


The value $\sigma_{X,r}(p)$ is the area of the intersection of the
ball $B_r(p)$ with shape $X$. By approaching the shape at
point $p \in X$, one can rewrite the intersection area
$\sigma_{X,r}(p)$ in the form of the Taylor expansion
\cite{pottman09intinvariant}:
\begin{align*}
  \sigma_{X,r}(p) = \frac{\pi}{2}r^2 - \frac{\kappa(X,p)}{3}r^3 + O(r^4),
\end{align*}
		
where $\kappa(X,p)$ is the curvature of $X$ at point $p$. By isolating $\kappa$ we can define a curvature estimator
	
\begin{align}
  \tilde{\kappa}(p) \coloneqq \frac{3}{r^3}\left( \frac{\pi r^2}{2} - \sigma_{X,r}(p) \right),
  \label{eq:curvature_approximation}
\end{align}
	
Such an approximation is convenient as one can simply devise a multigrid convergent estimator for the area.

\begin{definition}	
  Given a digital shape $D \subset (h \mathbb{Z})^2$,  the area estimatir $\widehat{Area}(D,h)$ is defined as	
  \begin{align}
    \widehat{Area}(D,h) \coloneqq h^2\text{Card}\left( D \right).	
    \label{eq:digital_estimator_area}
  \end{align}
\end{definition}
It is well known that this area estimator is multigrid convergent
(e.g. see \cite{klette2000multigrid}).  In
\cite{coeurjolly13integral}, the authors combine the
approximation\eqref{eq:curvature_approximation} and digital estimator
\eqref{eq:digital_estimator_area} to define a multigrid convergent
estimator for the curvature.

\begin{definition}[Integral Invariant Curvature Estimator]
  Let $D \subset (h \mathbb{Z})^2$ a digital shape. The integral invariant curvature estimator is defined as
  \begin{align*}
    \hat{\kappa}_{r}(D,x,h) \coloneqq \frac{3}{r^3} \left( \frac{\pi r^2}{2} - \widehat{Area} \left( B_{r} ( x ) \cap D, h \right) \right).
    %% \hat{\kappa}_{r}(D,x,h) \coloneqq \frac{3}{r^3} \left( \frac{\pi r^2}{2} - \widehat{Area} \left( B_{r/h} ( \frac{1}{h} \cdot x ) \cap D, h \right) \right).
  \end{align*}
\end{definition}

This estimator is multigrid convergent with speed $O(h^\frac{1}{3})$
for radii chosen as $r=\Theta(h^\frac{1}{3})$.  This estimator is also
robust to noise and can be extended to estimate the mean curvature of
three dimensional shapes.

\subsection{Elastica energy estimator}

In the remaining of this paper we are going to be interested in the minimization of the elastica energy. Given a Euclidean shape $X$ with smooth enough boundary, the elastica is defined as
\begin{align}
  E(X) = \int_{\partial X}{(\alpha + \beta \kappa^2) ds}, \quad \text{for~} \alpha \ge 0, \beta \ge 0.
  \label{eq:elastica}
\end{align}

The digital version of elastica energy approaching the continuous elastica energy is defined as follows (it uses multigrid convergent estimators):
\begin{align}
	\hat{E}( D_h(X) ) = \sum_{\vec{e} \in \partial D_h(X)}{ \hat{s}(\vec{e})\left(\; \alpha + \beta \hat{\kappa}_{r}^2(D_h(X),\dot{\vec{e}},h) \; \right)},
	\label{eq:digital-energy}
\end{align}
where $\dot{\vec{e}}$ denotes the center of the edge $\vec{e}$. In the
expression above, we will substitute an arbitrary subset $Z$ of
$\mathbb{Z}^2$ to $D_h(X)$ since the continuous shape $X$ is unknown.
In the following we omit the grid step $h$ to simplify expressions
(or, putting it differently, we assume that the shape of interest is
rescaled by $1/h$ and we set $h=1$).

\section{Local combinatorial optimization}

The objectives of this section is to show both the potential of minimizing a purely digital elastica energy but also to underline the difficulties of using it in a global combinatorial optimization framework.

Given a digital shape $S^{(0)}$ we describe a process that generates a
sequence $S^{(i)}$ of shapes with non-increasing elastica energy. The
idea is to define a neighborhood of shapes $\mathcal{N}^{(i)}$ to the
shape $S^{(i)}$ and choose the element of $\mathcal{N}^{(i)}$ with
lowest energy.  The process is suited for the integral invariant
estimator but also for other curvature estimators, for example, MDCA
\cite{roussillon11mdca}. As a matter of fact, our experiments have
shown that either estimators induce similar results.

Let $S$ be a $2$-dimensional digital shape. We adopt the cellular-grid model to represent $S$, i.e., pixels and its lower dimensional counterparts, linels and pointels, are part of $S$. In particular, we denote by $\partial S$ the topological boundary of $S$, i.e., the connected sequence of linels such that for each linel we have one of its incident pixels in $S$ and the other not in $S$.


Let $d_{S}:\Omega \rightarrow \mathcal{R}$ be the Euclidean distance transformation with respect to shape $S$. The value $d_S(x)$ gives the Euclidean distance between $x$ and the closest pixel in $S$. 

\begin{definition}[m-Ring Set]
Given a digital shape $S\in\Omega$, its distance transformation $d_S$ and natural numbers $m > 0$, the {\em $m$-ring set of $S$} is defined as
\begin{align*}
	\quad R_m(S) &:= R_m^-(S) \; \cup \; R_m^+(S) \\
	&:= \left\{ x \in \Omega \; | \; m-1 < d_S(x) \leq m \right\} \; \cup \;  \left\{ x \in \Omega \; | \; 	m-1 < d_{\overline{S}}(x) \leq m \right\}.
\end{align*}
\end{definition}

Consider the following set of neighbor candidates to $S$:
\begin{align*}
\mathcal{U}(S) = \{ D \; | D \subset R_1(S) \cup S \; \text{and} \; \text{$D$ is connected} \}.
\end{align*}


Such set can be extremely large and its complete exhaustion is prohibitively expensive.  Instead, we explore a subset of it with the help of $n$-glued curves.

An oriented closed curve $C$ is a closed connected sequence of linels with a well-defined interior. A segment of $C$ is a connected subsequence $c \in C$ of its linels.

\begin{figure}[!h]
\center
	\subfloat[\label{}]{%
	\includegraphics[scale=0.2]{images/local_search/definitions/model-regions-square.eps}
	}\hspace{40pt}%
	\subfloat[\label{}]{%
	\includegraphics[scale=0.2]{images/local_search/definitions/glued-curve.eps}
	}%	
	\caption{The blue pixels in figure (a) illustrates a $2$-ring set, while the green pixels its inner-pixel boundary. In figure (b) we highlight an element of set $\mathcal{G}_{11}(C_1,C_2)$. The curve segments of $C_1$ and $C_2$ are colored in red and green, while the junction linels in blue.}
\end{figure}

\begin{definition}[Glued Curve]
Given closed curves $C_1,C_2$ agreeing with some orientation $q$, a glued curve is a closed curve  $(c_1,\ell_1,c_2,\ell_2)$ with orientation $q$ and $c_1 \in C_1, c_2 \in C_2$. The linels $\ell_1,\ell_2$ are called junction linels.
\end{definition}

\begin{definition}[$n$-Glued Curve Set]
Given closed curves $C_1,C_2$ with same orientation, its set of $n$-glued curves is defined as
\begin{align*}
	\mathcal{G}_n(C_1,C_2) = \{ (c_1,\ell_1,c_2,\ell_2) \; | \; |c_2|=n \},
\end{align*}
\end{definition}

Let $S_O = ( S \cup R_1^+(S) ) $ and $S_I = ( S \setminus R_1^-(S) ) $, the neighborhood set to shape $S$ is defined as
\begin{align*}
	\mathcal{N}(S,N) = \bigcup_{1 \leq n \leq N} int \big( \mathcal{G}_{n}(\partial S_O, \partial S) \; \big) \cup int \big( \;  \; \mathcal{G}_{n}(\partial S, \partial S_I) \; \big),
\end{align*}

where $int(C)$ is the interior of the shape bounded by the close oriented curve $C$. Algorithm~\ref{alg:local-search} describes the local combinatorial process and Figure~\ref{fig:local-comb-square-results} presents the digital curve evolution when executing this algorithm for two different shapes with $N=20$.


\begin{algorithm}
 \SetKwData{It}{i}
 \SetKwData{MIt}{maxIt}
 \SetKwData{Tol}{tolerance}
 \SetKwData{Delta}{delta}
 \SetKwData{Best}{best} 
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 
 \Input{A digital set $S$; the maximum length of glued curves $N$; the maximum number of iterations \MIt; and a stop condition \Tol}
 \BlankLine
 \Delta $\longleftarrow$ \Tol+1\;
 \While{ \It $<$ \MIt \bf{and} \Delta $>$ \Tol  }{
  	\For{$ X \in \mathcal{N}(S^{(i)},N) $}
	{
		\If{ $\hat{E}(X)$ $<$ $\hat{E}(X^\star)$ }
		{
			$X^\star \longleftarrow X$
		}
	}
	\It $\longleftarrow$ \It $+1$\;
	$S^{(i)} \longleftarrow X^\star$\;
	\Delta $\longleftarrow$ $\hat{E}(S^{(i-1)}) - \hat{E}(S^{(i)})$\;	
 }
 \label{alg:local-search} 
 \caption{Local combinatorial optimization for elastica minimization.}
\end{algorithm}

		
\begin{figure}[!h]
\center
	\subfloat[h=1.0\label{}]{%
	\includegraphics[scale=0.125]{images/local_search/square/h1/summary-flow.eps}
	}%
	\hspace{10pt}
	\subfloat[h=0.5\label{}]{%
	\includegraphics[scale=0.125]{images/local_search/square/h05/summary-flow.eps}
	}%
	\hspace{10pt}
	\subfloat[h=1.0\label{}]{%
	\includegraphics[scale=0.125]{images/local_search/flower/h1/summary-flow.eps}
	}%
	\hspace{10pt}
	\subfloat[h=0.5\label{}]{%
	\includegraphics[scale=0.125]{images/local_search/flower/h05/summary-flow.eps}
	}%
		\caption{Local combinatorial optimization process results for the square and flower shapes. Shapes displayed at every $2$ iterations for the square and $7$ iterations for the flower.}	
		\label{fig:local-comb-square-results}
\end{figure}


The running time of algorithm \ref{alg:local-search} is summarized in
table \ref{tab:summary-local-comb-rtime}. All the experiments in this paper were executed on a five core $3.4Ghz$ cpu and the number of pixels in the triangle, square and flower shapes are respectively $841,1867,521$ for grid step $h=1.0$. Although its use in practical applications is limited, we
demonstrate that digital estimators are effective in their measurements
and the flows evolve as expected. We observe that it is a
complete digital approach, and we do not suffer from discretization
and rounding problems, a common issue in continuous models.
Furthermore we have checked that this approach works
indifferently with Integral Invariant curvature estimator and
Maximal Digital Circular Arc curvature estimator. So the convergence
of the digital curvature estimator seems to be the cornerstone to
get a digital curve behaving like a continuous elastica.  In the
next section, we explore a more efficient approach.



\begin{figure}[!h]
\begin{minipage}[b]{0.5\textwidth}
\center
\includegraphics[scale=0.35]{images/local_search/square-flower.eps}
\caption{Elastica computation along iterations of algorithm \ref{alg:local-search}}
\label{fig:plot-elastica-local-search}
\end{minipage}\hspace{40pt}%
\begin{minipage}[b]{0.45\textwidth}
\center
\captionsetup{type=table}
\begin{tabular}{|l|c|c|}
\hline
& $h=1.0$ & $h=0.5$\\
\hline
Square & 47s (5s/it) & 260s (12s/it)\\
\hline
Flower & 1235s (23s/it)  & 8516s (52s/it)\\
\hline
\end{tabular}
\caption{Running times for the local combinatorial optimization algorithm with $k=1,N=20$. Four threads were used.}
\label{tab:summary-local-comb-rtime}
\end{minipage}
\end{figure}

\section{Digital Curvature Flow}

%	Let $\Omega_C$ be the collection of connected digital sets in $\Omega$. Given a digital shape $S$, we wish to evolve it into a shape $S^\star$ similar to $S$ but with lower digital elastica energy. We recall the digital curvature estimator definition
%	
%\begin{align}
%	\hat{\kappa}^2(y) &= c_1\Big( c_2 - | B_r(y) \cap S | \Big)^2, 
%	\label{eq:curvature-estimator-pixels}
%\end{align}
%
%where $c_1=3/r^6$ and $c_2=\pi r^2/2$. Let $X,Y$ be the set of pixels and linels, respectively, in $\Omega$. We define the following binary model
%		
%\begin{align}
%\text{Minimize}_{x,y} & \sum_{y}{c_1y \Big(c_2 - \sum_{x_j \in B_r(y)}{x_j}\Big)^2	 } + F(S,X_{=1} \cup Y_{=1}) \nonumber \\
%\text{subject to} & \nonumber \\
%& (X_{=1} \cup Y_{=1}) \in \Omega_C, \; x,y \in \{0,1\}^\Omega
%\label{eq:general-binary-formulation}
%\end{align}
%
%
%where $F$ is some measure of similarity, e.g. distance, between initial shape $S$ and candidate solution. Such term is needed, otherwise the uninteresting empty set would be the optimal solution.
%
%Formulation \eqref{eq:general-binary-formulation} is of order four, binary and non-convex. These properties make it a hard optimization problem. We are going to explore an alternative energy, which is a simplification of the former.

%\subsection{A first model}
%
%We note that the elastica can be decreased by attenuating non-flat regions. The idea is to evolve a flow $S^{(i)}$ where $S^{(i+1)}$ is derived from the minimization of the sum of the squared curvature at $S^{(i)}$. We exchange solving a fourth order energy to solve multiple second order problems. We still have to guarantee that solutions are connected. We handle this issue by limiting the optimization region to a subset of $\Omega$, namely the inner pixel boundary of  $S^{(i)}$.





Given a digital shape $S \in \Omega$, the idea is to evolve a flow $S^{(i)}$ where $S^{(i+1)}$ is derived from the minimization of the sum of the squared curvature at $S^{(i)}$. In order to guarantee connectivity, we limit the optimization region to a subset of $\Omega$, namely the inner pixel boundary of  $S^{(i)}$. We also use the notation $| S |$ to denote the cardinal of a digital set $S$.

\subsection{Discrete variational model for discrete curve evolution}

We start by definition the optimization variables.
\begin{definition}[Inner pixel boundary]
Given a digital shape $S$ embedded in a domain $\Omega$, we define its inner pixel boundary set $O(S)$ as
\begin{align*}
	O(S) = \left\{ \: x \; | \; x \in S, |\mathcal{N}_4(x) \cap S|<4 \: \right\},
\end{align*}
where $\mathcal{N}_4(x)$ denotes the $4$-adjacent neighbor set of $x$ (without $x$).
\end{definition}

In other words, each flow iteration decides which pixels in the inner boundary are to be removed and which are to be kept. To simplify notation, we denote the optimization region of $S^{(i)}$ as $O^{(i)}$, and the foreground set as $F^{(i)} = S^{(i)} \setminus O^{(i)}$. We recall the definition of our digital curvature estimator :
\begin{align}
	\hat{\kappa}^2(y) &= c_1\Big( c_2 - | B_r(y) \cap S | \Big)^2, 
	\label{eq:curvature-estimator-pixels}
\end{align}
where $c_1=3/r^6$ and $c_2=\pi r^2/2$. 

We define $O_{r}^{(i)}(y) := O^{(i)} \cap B_r(y)$ and an analogous definition holds for $F_{r}^{(i)}(y)$. Expanding \eqref{eq:curvature-estimator-pixels}, we get 
\begin{align*}
  \hat{\kappa}^2(y) &= c_1\Big( c_2 - \sum_{x_j \in B_r(y)} \big( {x_j} + |F_{r}^{(i)}(y)| \big) \Big)^2 \\
   &= c_1 \cdot \Big( C + 2\left( |F_{r}^{(i)}(y)| - c_2 \right) \hspace{-2mm}\sum_{x_j \in O_{r}^{(i)}(y)}\hspace{-2mm}{x_j} + \hspace{-2mm}\sum_{x_j \in O_{r}^{(i)}(y)}\hspace{-2mm}{x_j^2} + \hspace{-2mm}\sum_{ \substack{x_j,x_k \in O_{r}^{(i)}(y) \\ j<k} }\hspace{-2mm}{2x_jx_k}  \Big),
\end{align*}
where $C=c_2^2 - 2c_2 \cdot |F_{r}^{(i)}(y)| + |F_{r}^{(i)}(y)|^2$ is a constant. By ignoring constants and multiplication factors and using
the binary character of the variables, we define the following family
of energies for given parameters $\alpha,\beta, \gamma \geq 0$.
\begin{align}
  E_m(X,S) =& \sum_{x \in X}{\alpha s(x)} \nonumber \\ 
  & + \sum_{y \in R_m(S)}{ \beta \Big( { (1/2+ |F_{r}^{(i)}(y)|-c_2) \cdot \sum_{x_j \in O_{r}^{(i)}(y)}{x_j} } + \sum_{ \substack{x_j,x_k \in O_{r}^{(i)}(y) \\ j<k} }{x_jx_k} \Big) } \nonumber \\
  & + \sum_{x \in X}{\gamma g(S,x)},
  \label{eq:energy-family}
\end{align}
where $g(S,x)$ denotes a similarity term as distance to initial shape $S$ or data fidelity term and $s(x)$ denotes a length penalization term. Each choice of $m$ generates a different flow, which is generaly described in the Digital Curve Evolution (DCE) algorithm \ref{alg:evolution-model}.


\begin{algorithm}[H]
 \SetKwData{It}{i}
 \SetKwData{MIt}{maxIt}
 \SetKwData{Delta}{delta}
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \SetKwComment{comment}{//}{}
 
 \Input{A digital set $S$; The ring number $m$; Length($\alpha$), curvature($\beta$) and similarity($\gamma$) coefficients; the maximum number of iterations \MIt;}
 \BlankLine
 $S^{(i)} \longleftarrow S$\;
 \While{ \It $<$ \MIt  }{ 	
	\comment{Expansion mode}
	\If{ \It is odd }{	 
	 	$X^{(i)} \longleftarrow \argmin_X E_m(X,\overline{S^{(i)}})$\;
 		$S^{(i)} \longleftarrow F^{(i)} + \overline{X^{(i)}}$\;
 		$S^{(i)} \longleftarrow \overline{S^{(i)}}$\;	 	
 	}
	\comment{Shrinking mode} 	
 	\Else{	
	 	$X^{(i)} \longleftarrow \argmin_X E_m(X,S^{(i)})$\; 	
 		$S^{(i)} \longleftarrow F^{(i)} + \overline{X^{(i)}}$\;
 	}
 	
	\It $\longleftarrow$ \It $+1$\;
	
 }
 \label{alg:evolution-model} 
 \caption{Digital curvature evolution algorithm (DCE).}
\end{algorithm}



Recall that the integral invariant estimator approaches curvature by
computing the difference between half of the area of a chosen ball and
the area of the intersection of this ball with the shape.  In
particular, regions of positive curvature have fewer pixels in their
intersection set than on its complement wrt the estimation ball. This
implies that variables in such regions are labeled one, as the
unbalance grows otherwise. We attenuate curvature if we shift the
center of the estimation ball towards the interior of the shape, which
means removing the labeled one pixels. That is why we take the
complement of the optimization solution.

The same reasoning applies for non-convex parts. Indeed, concave
regions are convex in the shape complement. In the expansion mode we
apply the same reasoning on the image complement, and by doing this we
are able to handle concavities. It is called expansion mode because
the optimization region, in this case, is the outer pixel boundary of
the original shape. Table~\ref{tab:flow-summary} sums up these
arguments.

Length and similarity terms should be properly defined in order to
comply with the complement step of the DCE algorithm. The length
penalization is defined as
\begin{align}
  s(x)=\sum_{x_j \in \mathcal{N}_4(x)}{ t(x_j) }, \quad \text{where } t(x_j) = \left\{\begin{array}{ll}
  (x-x_j)^2, & \text{if } x_j \in O_{r}^{(i)}\\
  (x-0), & \text{if } x_j \in F_{r}^{(i)}\\
  (x-1), & \text{otherwise }
  \end{array}\right.
  \label{eq:length-penalization}
\end{align}
	
We do not use similarity terms in this section, thus we postpone the
definition of such terms until the description of how DCE can be used
in an image segmentation framework.



\begin{table}
  \center
  \begin{tabular}{|c|c|c|c|} \hline
    shrink mode &    $\kappa \gg 0$ & $\kappa \geq 0$ &  $\kappa < 0$ \\ \hline
    $X$ & $x_k=1$ & $x_k \in \{0,1\}$ & $x_k=0$ \\ \hline
    $S^{(i+1)} \leftarrow S^{(i)} \setminus X$ & eroded & prob. eroded & unchanged  \\ \hline \hline
    expansion mode &    $\bar{\kappa} \gg 0$ & $\bar{\kappa} \geq 0$ & $\bar{\kappa} < 0$ \\ \hline
    $\bar{X}$ & $\bar{x}_k=1$ & $\bar{x}_k \in \{0,1\}$ & $\bar{x}_k=0$ \\ \hline
    $S^{(i+1)} \leftarrow \overline{\bar{S}^{(i)} \setminus \bar{X}}$ & dilated & prob. dilated & unchanged \\ \hline 
  \end{tabular}
  
  \caption{  Since the curvature is negated when reversing the curve (i.e. $\bar{\kappa}=-\kappa$), this process can only shrink  convex parts in shrink mode and expand concave parts in expansion mode.}
   \label{tab:flow-summary}	  

\end{table}


In figure \ref{fig:m1-square-flow} we see some results for the DCE
algorithm with $m=1$. We observe a global movement towards roundness,
but with several artifacts. We minimize the effects of a jaggy
boundary by setting $\alpha > 0$. Nonetheless, a higher estimation
ball radius creates unstable shapes. In fact, the estimator is very
sensitive in regions of low squared curvature, and it is precisely in
those regions that spurious pixels are created. 


\begin{figure}[!h]
\center
\begin{minipage}[b]{0.33\textwidth}
	\subfloat[$r=3, \alpha=0$\label{}]{%
	\includegraphics[scale=0.2]{images/flow/length-radius-effect/triangle-r3-nolength/summary_flow.eps}
	}%
\end{minipage}%
\begin{minipage}[b]{0.33\textwidth}
	\subfloat[$r=3, \alpha=0.15$\label{}]{%
	\includegraphics[scale=0.2]{images/flow/length-radius-effect/triangle-r3-length0.15/summary_flow.eps}
	}%
\end{minipage}%
\begin{minipage}[b]{0.33\textwidth}
	\subfloat[$r=5, \alpha=0.15$\label{}]{%
	\includegraphics[scale=0.2]{images/flow/length-radius-effect/triangle-r5/summary_flow.eps}
	}%
\end{minipage}%
\caption{The algorithm is very sensitive to the little variations of the estimator, which are particularly important in regions of low squared curvature. Artifacts are somewhat reduced with a length penalization but increases if we use a higher ball radius. }
\label{fig:m1-square-flow}
\end{figure}


\subsection{A more stable model.}
In the previous section we noticed that the algorithm produces shapes
with many artifacts due to the small uncertainties of the estimator
along regions of low squared curvature. We argue that, evaluating the
estimation ball along farther ring sets we avoid those sensitive areas
by focusing the optimization process only on regions with highest
squared curvature value. 

%%\jaco{Motivation in digital geometry of why it makes sense to compute things farther.}



\begin{figure}[!h]
\center
\center
\begin{minipage}[b]{0.33\textwidth}
	\subfloat[$r=5, m=1$\label{}]{%
	\includegraphics[scale=0.2]{images/flow/farther-rings/square-r5-l1/summary_flow.eps}
	}%
\end{minipage}%
\begin{minipage}[b]{0.33\textwidth}
	\subfloat[$r=5, m=2$\label{}]{%
	\includegraphics[scale=0.2]{images/flow/farther-rings/square-r5-l2/summary_flow.eps}
	}%
\end{minipage}%
\begin{minipage}[b]{0.33\textwidth}
	\subfloat[$r=5, m=3$\label{}]{%
	\includegraphics[scale=0.2]{images/flow/farther-rings/square-r5-l3/summary_flow.eps}
	}%
\end{minipage}%
\caption{By positioning the estimation ball on farther rings, we minimize artifacts creation. \label{fig:mx-square-flow}}
\end{figure}

\daniel{In our experiments, the best results are obtained by executing DCE
algorithm with $m$ equal to $r$, where $r$ is the estimation ball
radius (see figure \ref{fig:mx-square-flow} ). We observe that elastica may increase after some iterations if chosen radius is too large, as in the case of the triangle in figure \ref{fig:mx-flow-gs-radius-effect} in which the flow converges to a single point. We conjecture that an appropriated value for the radius should be given by the shape reach.
	The produced flow has no difficulties in handling changes on topology, and it presents different speeds for regions with low and high curvature values, as illustrated in figure \ref{fig:mx-speed-variation-hole-filling}.}

\begin{figure}[hp!]
\center				
		\setcounter{subfigure}{-3}
		\subfloat[$r=3,h=0.5$]{%
		\subfloat{%
		\includegraphics[scale=0.2]{images/flow/grid-radius-effect/triangle/r3-h0.5/summary_flow.eps}
		}%
		\hspace{15pt}
		\subfloat{%
		\includegraphics[scale=0.15]{images/flow/grid-radius-effect/square/r3-h0.5/summary_flow.eps}}%
		\hspace{15pt}
		\subfloat{%
\includegraphics[scale=0.2]{images/flow/grid-radius-effect/flower/r3-h0.5/summary_flow.eps}}%
		}		
	
		\setcounter{subfigure}{-2}
		\subfloat[$r=5,h=0.5$]{%
		\subfloat{%
		\includegraphics[scale=0.2]{images/flow/grid-radius-effect/triangle/r5-h0.5/summary_flow.eps}
		}%1
		\hspace{15pt}
		\subfloat{%
		\includegraphics[scale=0.15]{images/flow/grid-radius-effect/square/r5-h0.5/summary_flow.eps}}%
		\hspace{15pt}
		\subfloat{%
\includegraphics[scale=0.2]{images/flow/grid-radius-effect/flower/r5-h0.5/summary_flow.eps}}%
		}\\
		\subfloat[Elastica evaluation]{%
		\includegraphics[scale=0.5]{images/flow/elastica-energy-plot/r5h05.eps}						
		}
\caption{The choice of radius impacts the flow. In the figures, the flow ceases to evolve for all shapes  when $r=3$ (a). In figure (b), for $r=5$, the triangle evolves to a single point, while the others stop  in an intermediate shape, as in (a). In figure (c), we observe that for a given choice of radius, the elastica may increase after a certain number of iterations. }
\label{fig:mx-flow-gs-radius-effect}
\end{figure}


\begin{figure}[!h]
\center
\subfloat[]{%
	\includegraphics[scale=0.5]{images/flow/faster-high-curvature/summary_flow.eps}}\\%
\subfloat[]{%
		\includegraphics[scale=0.25]{images/flow/fill-holes/summary_flow.eps}}%%1
\caption{High curvature regions evolves faster than lower ones (a). The flow can handle topological changes (b). }
\label{fig:mx-speed-variation-hole-filling}
\end{figure}


\subsection{Optimization method}

Let $f$ be a function of $n$ binary variables with unary and pairwise terms, i.e.

\begin{align*}
f(y_1,\cdots, y_n) = \sum_{j}{f_j(y_j)} + \sum_{j < k}{f_{j,k}(y_j,y_k)}.
\end{align*}

Function $f$ is submodular if and only if the following inequality holds for each pairwise term $f_{j,k}$ \cite{kolmogorov04whatenergies}:
\begin{align*}
  \quad f_{j,k}(0,0) + f_{j,k}(1,1) \leq f_{j,k}(0,1) + f_{j,k}(1,0).
\end{align*}

Energy $E_m$ is non-submodular and optimizing it is a difficult
problem, which constrains us to use heuristics and approximation
algorithms. The QPBO method \cite{rother07qpbo} transforms the
original problem in a max-flow/min-cut formulation and returns a full
optimal labeling for submodular energies. For non-submodular energies
the method is guaranteed to return a partial labeling with the
property that the set of labeled variables is part of an optimal
solution. That property is called partial optimality.

In practice, QPBO can leave many pixels unlabeled. There exist two
extensions of QPBO that alleviate this limitation: QPBOI (improve) and
QPBOP (probe). The first is an approximation method that is guaranteed
to not increase the energy, but we lost the property of partial
optimality. The second is an exact method which is reported to label
more variables than QPBO.

The percentage of unlabeled pixels by QPBOP for $E_1$ is quite high,
but the percentage decreases to zero as we set $m$ equal to
$r$. Therefore, we are more confident to take the solution for values
of $m$ close to $r$. However, the way it varies across $m$ values
differs from shape to shape, as is illustrated in figure
\ref{fig:unlabeled-versus-iterations}. We also noticed that, for
$m=r$, all the pixels were labeled, \daniel{which may indicate that $E_r$ is an an easy 
instance of the general non-submodular energy $E_m$, but this remains to be proved. The number of pairwise
terms in $E_r$ is roughly half of those in $E_1$ (see figure \ref{fig:ratio-pairwise-terms}).}

We have used QPBOI to solve $E_m$. Naturally, in the case where all
pixels are labeled by QPBOP, QPBOI returns the same labeling as QPBOP.


\begin{figure}[h!]
\center
\includegraphics[scale=0.5]{images/optimization/pairwise-ratio/plot-pairwiseratio-lowerHigher-concavities-probe.eps}
\caption{We plot the ratio of pairwise terms among all $\binom{|X^{(i)}|}{2}$ combinations. The highest ring has rougly half the number of pairwise terms  than the lowest ring.}
\label{fig:ratio-pairwise-terms}
\end{figure}


\begin{figure}[hp!]
\center
\begin{minipage}[b]{0.5\textwidth}
\subfloat[]{ \includegraphics[scale=0.35]{images/optimization/unlabeled-iterations/radius-3/plot-model-square-concavities-probe.eps}}\\%
\subfloat[]{ \includegraphics[scale=0.35]{images/optimization/unlabeled-iterations/radius-5/plot-model-square-concavities-probe.eps}}\\%
\subfloat[]{ \includegraphics[scale=0.35]{images/optimization/unlabeled-iterations/radius-7/plot-model-square-concavities-probe.eps}}%
\end{minipage}%
\begin{minipage}[b]{0.5\textwidth}
\subfloat[]{ \includegraphics[scale=0.35]{images/optimization/unlabeled-iterations/radius-3/plot-model-flower-concavities-probe.eps}}\\%
\subfloat[]{ \includegraphics[scale=0.35]{images/optimization/unlabeled-iterations/radius-5/plot-model-flower-concavities-probe.eps}}\\%
\subfloat[]{ \includegraphics[scale=0.35]{images/optimization/unlabeled-iterations/radius-7/plot-model-flower-concavities-probe.eps}}%
\end{minipage}
\caption{For each plot, we first produce shapes $\left\{ S^{(i)} \right\}$ executing DCE with $m=r$. Then, for each shape in $\left\{ S^{(i)} \right\}$, we execute one iteration of DCE for different values of $m$ and we count the unlabeled pixels. The number of unlabeled pixels by QPBOP remains high for lower values of $m$, and goes to zero when $m=r$. We observe the same behaviour for different radius values.}
\label{fig:unlabeled-versus-iterations}
\end{figure}

\section{Application to image segmentation}

We present an application of our digital curve evolution algorithm to supervised image segmentation. The DCE will act as a contour correction method. Here we are going to use a data fidelity term in order to characterize the object of interest. Given foreground and background seeds selected by the user, we derive mixed gaussian distributions  of color intensities $G_f$ and $G_b$, and we define the data fidelity term as
	
\begin{align}
  g(y) = -(1-y)\log{G_f(I(y))} - y\log{G_b(I(y))}.
  \label{eq:data-fidelity}
\end{align}	

We use the DCE algorithm to regularize an initial contour outputed by some segmentation algorithm or delineated by the user. In this application, the similarity term of the DCE
is set to the data fidelity term \eqref{eq:data-fidelity}.
	
\begin{algorithm}[H]
 \SetKwData{It}{i}
 \SetKwData{MIt}{maxIt}
 \SetKwData{Tol}{tolerance}
 \SetKwData{Delta}{delta}
 \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
 \SetKwComment{comment}{//}{}
 
 \Input{An image $I$; seed mask $M$; the estimation ball radius $r$; length $(\alpha)$, squared curvature $(\beta)$ and data fidelity $(\gamma)$ coefficients; initial dilation $d$; stop condition value \Tol; the maximum number of iterations \MIt;}
 \BlankLine

 $S \longleftarrow$ GrabCut($I,M$)\;
 $S^{(0)} \longleftarrow $ dilate($S$,$d$)\; 
 \Delta $\longleftarrow +\infty$\;
 $i \longleftarrow 0$\;
 \While{ \It $<$ \MIt \bf{and} \Delta $>$ \Tol  }{ 	
 	$S^{(i+1)} \longleftarrow $ DCE($S^{(i)},r,\alpha,\beta,\gamma,2$)\;
 	\Delta $\longleftarrow |S^{(i)} - S^{(i+1)}|$\;

	\It $\longleftarrow$ \It $+1$\;
	
 }
 \label{alg:contour-correction} 
 \caption{Contour correction algorithm.}
\end{algorithm}	

The algorithm can be initialized by a collection of compact sets, or with the result of a third-party segmentation algorithm, as GrabCut \cite{rother04grabcut}. We include an additional parameter $d$ that dilates the initial sets using a square of side one before executing the flow.
	
\begin{figure}[ht!]
\center
\begin{tabular}{ccc}
Seeds & $(\alpha=0, \beta=0.5)$ & $(\alpha=0,\beta=1)$ \\
 	\includegraphics[scale=0.25]{images/segmentation/bc/coala/seeds.png} & 
	\includegraphics[scale=0.25]{images/segmentation/bc/coala/r3/lg0_sq05_dt1_it50.png} & 
	\includegraphics[scale=0.25]{images/segmentation/bc/coala/r3/lg0_sq1_dt1_it50.png} \\
	
 	\includegraphics[scale=0.25]{images/segmentation/bc/coala/lg0_sq0_dt1_it20.png} & 
	\includegraphics[scale=0.25]{images/segmentation/bc/coala/r3/lg1_sq0_dt1_it50.png} &
	\includegraphics[scale=0.25]{images/segmentation/bc/coala/r3/lg2_sq0_dt1_it50.png} \\

	GrabCut & $(\alpha=0.5, \beta=0)$ & $(\alpha=1, \beta=0)$
\end{tabular}	
\caption{Comparison of squared curvature regularization (first row) and length regularization (second row). }
\label{fig:parameters-influence}
\end{figure}

\begin{figure}[ht!]
	\center
	\begin{tabular}{ccc}
		GrabCut & Contour Correction & Schoenemann \\
		& $(r=5, \alpha=0.1, \beta=1.0, \gamma=3.0$) & $(\alpha=0.1, \beta=1.0$)\\
		\includegraphics[scale=0.2]{images/segmentation/bc/tiger1/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/tiger1/corrected-seg.png} &					\includegraphics[scale=0.2]{images/segmentation/schoenemann/tiger1/tiger1-seg.png}\\									
		\includegraphics[scale=0.2]{images/segmentation/bc/snake/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/snake/corrected-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/schoenemann/snake/snake-seg.png}\\						
		\includegraphics[scale=0.2]{images/segmentation/bc/man/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/man/corrected-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/schoenemann/man/man-seg.png}\\		
		\includegraphics[scale=0.2]{images/segmentation/bc/vase/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/vase/corrected-seg.png} &					\includegraphics[scale=0.2]{images/segmentation/schoenemann/vase/vase-seg.png}		
	\end{tabular}
	\caption{The proposed method regularizes GrabCut contours and returns meaningful results. We can observe the completion feature of curvature in the second row, and we don't suffer from oversegmentation issues as Schoenemann's method. However, our flow may stop in a local optima as in the fourth row, while Schoenemann's is able to extrapolate such solutions.}
	\label{fig:segmentation-results}	
\end{figure}


\begin{figure}[hp!]
	\center
	\begin{tabular}{ccc}
		GrabCut & Contour Correction & Schoenemann \\
		& $(r=5, \alpha=0.1, \beta=1.0, \gamma=3.0$) & $(\alpha=0.1, \beta=1.0$)\\
		\includegraphics[scale=0.2]{images/segmentation/bc/airplane2/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/airplane2/corrected-seg.png} &					\includegraphics[scale=0.2]{images/segmentation/schoenemann/airplane2/airplane2-seg.png}\\									
		\includegraphics[scale=0.2]{images/segmentation/bc/bird/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/bird/corrected-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/schoenemann/bird/bird-seg.png}\\						
		\includegraphics[scale=0.2]{images/segmentation/bc/camel/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/camel/corrected-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/schoenemann/camel/camel-seg.png}\\		
		\includegraphics[scale=0.2]{images/segmentation/bc/rock/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/rock/corrected-seg.png} &					\includegraphics[scale=0.2]{images/segmentation/schoenemann/rock/rock-seg.png}\\
		\includegraphics[scale=0.2]{images/segmentation/bc/giraffes/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/giraffes/corrected-seg.png} &					\includegraphics[scale=0.2]{images/segmentation/schoenemann/giraffes/giraffes-seg.png}\\
		\includegraphics[scale=0.2]{images/segmentation/bc/canguru/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/canguru/corrected-seg.png} &					\includegraphics[scale=0.2]{images/segmentation/schoenemann/canguru/canguru-seg.png}\\		
		\includegraphics[scale=0.2]{images/segmentation/bc/coral/gc-seg.png} &
		\includegraphics[scale=0.2]{images/segmentation/bc/coral/corrected-seg.png} &					\includegraphics[scale=0.2]{images/segmentation/schoenemann/coral/coral-seg.png}\\				
						
	\end{tabular}
	\caption{}
	\label{fig:more-segmentation-results}	
\end{figure}


We evaluate our method using the BSD300 database \cite{martinFTM01berkeley}. All images contains the same number of pixels, the resolution being 321x481 in portrait mode. We compare the results of our method with segmentations given by GrabCut and Schoenemanns's method \cite{schoenemann09linear}. We report an average of $3s$ per flow iteration, and an average of $30$ iterations per image. While GrabCut executes in less than one second, Schoenemann's method may take several hours to complete.


\daniel{In figure \ref{fig:parameters-influence} we can observe the results of a curvature regularization in comparison with a pure length regularization. The curvature can fill gaps and is smoother than the one produced by length only, resulting in more pleasant segmentations. In figures \ref{fig:segmentation-results} and \ref{fig:more-segmentation-results} we list several results of our method and we compare them with segmentation produced by GrabCut and Schoenemann's method. Our method is successful in producing curvature regularized segmentations and demonstrates the completion property of curvature. Moreover, it does not suffer from over segmentation, it is faster than Schoenemann's, and in several opportunities produces better segmentations than GrabCut.}

\section{Conclusion}\label{sec:conclusion}


We have studied in depth several digital curve evolution models based
on the elastica energy and we have presented an application of one of
them to image segmentation. The processes we have described are
completely digital, and do not suffer from issues that typically
arises in models that passes through a discretization stage, as
rounding. Moreover, the model can handle changes in topology and its
results are competitive with similar approaches while achieving
reasonable running times.

Future developments of this work may include automatization, no more
dependence on seeds selected by the user and automatic choice of ball
estimation radius based on the reach of the shape. One can also think
to extend the DCE algorithm to $3$d, as the integral invariant
estimator preserves its properties in that space. In future works, a
denoising application may be derived by running the flow on the $3$d
shape with data fidelity defined by image intensities or by distance
to initial position.



%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{jmiv}

\end{document}
